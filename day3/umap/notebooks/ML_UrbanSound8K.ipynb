{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35e8d1cf",
   "metadata": {},
   "source": [
    "# Supervised and Unsupervised Machine Learning Methods for Urban Sound dataset \n",
    "\n",
    "In this exercise, we will apply supervised and unsupervised machine learning techniques to classify urban sounds using the UrbanSound8K dataset. After extracting features from audio files, we will train a K-Nearest Neighbors (KNN) classifier and visualize the data using UMAP (Uniform Manifold Approximation and Projection). Next, we will use the same features to train a Convolutional Neural Network (CNN) and compare its performance to KNN. UMAP will also be used to visualize one of the CNN's last layers.\n",
    "\n",
    "<img src=\"../images/concept.png\" alt=\"Urban Sound Dataset\" width=\"600\" style=\"display: block; margin: auto;\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82e665d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "379feacd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0a2cc4",
   "metadata": {},
   "source": [
    "### Paths and devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb6f961f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = Path(\"..\") / \"data\"\n",
    "metadata_path = data_path / \"UrbanSound8K.csv\"\n",
    "\n",
    "# load device depending on your system\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")        # NVIDIA GPU\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")         # Apple Silicon\n",
    "else:\n",
    "    device = torch.device(\"cpu\")         # CPU fallback\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "# One liner:\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bc914a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(metadata_path)\n",
    "metadata.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e93b213d",
   "metadata": {},
   "source": [
    "Have a first look at the labels and the distribution of the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afe74c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 3))\n",
    "sns.countplot(\n",
    "    y=metadata[\"class\"], \n",
    "    order=metadata[\"class\"].value_counts().index, \n",
    "    palette=\"viridis\")\n",
    "plt.title(\"Classes with their counts\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "382fe1d7",
   "metadata": {},
   "source": [
    "### Audio processing parameters\n",
    "\n",
    "Librosa is a Python package for music and audio analysis.\n",
    "\n",
    "- https://librosa.org/doc/main/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c44358",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_processing import show_mel_augmentations\n",
    "show_mel_augmentations(metadata, data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d57bf7",
   "metadata": {},
   "source": [
    "## Extract features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abf2758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from audio_processing import extract_audio_features\n",
    "\n",
    "# Initialize the dictionary with only the keys you want to compute\n",
    "audio_dict = {\n",
    "    \"path\": [],\n",
    "    \"labels\": [],\n",
    "    \"class\": [],\n",
    "    \"train_test\": [],\n",
    "    \"raw\": [],\n",
    "    \n",
    "    # Toggle features by commenting/uncommenting\n",
    "    # \"rms\": [],\n",
    "    # \"spec_bw\": [],\n",
    "    # \"poly_features\": [],\n",
    "    \"spec_centroid\": [],\n",
    "    # \"spec_flatness\": [],\n",
    "    # \"spec_rolloff\": [],\n",
    "    \"mean_mfccs\": [],\n",
    "}\n",
    "\n",
    "# Get the set of keys to determine which features to extract\n",
    "feature_keys = set(audio_dict.keys())\n",
    "\n",
    "# Process each audio file in the dataset\n",
    "for i, row in tqdm(metadata.iterrows(), total=len(metadata), desc=\"Extracting audio features\"):\n",
    "    # Construct audio file path\n",
    "    audio_path = os.path.join(data_path, f\"fold{row['fold']}\", row[\"slice_file_name\"])\n",
    "    \n",
    "    # Extract only the features we need\n",
    "    features = extract_audio_features(audio_path, feature_keys)\n",
    "    \n",
    "    # Add metadata (always included)\n",
    "    audio_dict[\"path\"].append(audio_path)\n",
    "    audio_dict[\"labels\"].append(row[\"classID\"])\n",
    "    audio_dict[\"class\"].append(row[\"class\"])\n",
    "    audio_dict[\"train_test\"].append(\"train\" if row[\"fold\"] <= 8 else \"test\")\n",
    "    \n",
    "    # Add extracted features (only those that were computed)\n",
    "    for key, value in features.items():\n",
    "        audio_dict[key].append(value)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53ea42c",
   "metadata": {},
   "source": [
    "If needed, we can create a new feature vectors by concatenating the extracted features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcdfca21",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_dict[\"feature\"] = audio_dict[\"mean_mfccs\"]\n",
    "# audio_dict[\"feature\"] = audio_dict[\"spec_centroid\"]\n",
    "\n",
    "# Alternatively, you can concatenate multiple features into a single feature vector\n",
    "# audio_dict[\"feature\"] = []\n",
    "# for i in range(len(audio_dict[\"labels\"])):\n",
    "#     audio_dict[\"feature\"].append(\n",
    "#         np.concatenate([audio_dict[\"mean_mfccs\"][i], audio_dict[\"spec_centroid\"][i]])\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7897b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_df = pd.DataFrame(\n",
    "    audio_dict, columns=[\"path\", \"labels\", \"class\", \"train_test\", \"feature\"]\n",
    ")\n",
    "audio_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c9e530",
   "metadata": {},
   "source": [
    "#### Let's listen "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8fc2f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display\n",
    "import random\n",
    "from config import SR\n",
    "\n",
    "label_to_listen = 3\n",
    "\n",
    "# Find random index of the label to listen\n",
    "indices = [\n",
    "    i for i, num in enumerate(audio_dict[\"labels\"]) if num == label_to_listen\n",
    "]\n",
    "\n",
    "if indices:\n",
    "    random_index = random.choice(indices)\n",
    "    print(\"{} (index={})\".format(audio_dict[\"class\"][random_index], random_index))\n",
    "\n",
    "IPython.display.Audio(audio_dict[\"raw\"][random_index], rate=SR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95705b04",
   "metadata": {},
   "source": [
    "## Supervised Learning Methods using the extracted features\n",
    "\n",
    "Here, we will use the extracted features to train:\n",
    "- KNeighborsClassifier: K Nearest Neighbors Classifier\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html\n",
    "- RandomForestClassifier: Random Forest Classifier\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "- MLPClassifier: Multi-layer Perceptron Classifier\n",
    "    - https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b36148a",
   "metadata": {},
   "source": [
    "#### Split the dataset into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1b6c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_and_labels(df, feature_col=\"feature\", label_col=\"labels\"):\n",
    "    \"\"\"\n",
    "    Split a DataFrame into train and test sets based on the 'train_test' column.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing the data\n",
    "        feature_col: Column name for features\n",
    "        label_col: Column name for labels\n",
    "        \n",
    "    Returns:\n",
    "        X_train, y_train, X_test, y_test: NumPy arrays of features and labels\n",
    "    \"\"\"\n",
    "    # Extract train data\n",
    "    train_data = df[df[\"train_test\"] == \"train\"]\n",
    "    X_train = np.array(train_data[feature_col].tolist())\n",
    "    y_train = np.array(train_data[label_col].tolist())\n",
    "    \n",
    "    # Extract test data\n",
    "    test_data = df[df[\"train_test\"] == \"test\"]\n",
    "    X_test = np.array(test_data[feature_col].tolist())\n",
    "    y_test = np.array(test_data[label_col].tolist())\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "# Use the function to get train and test data\n",
    "X, y, X_test, y_test = split_features_and_labels(audio_df)\n",
    "\n",
    "# Print shapes to verify\n",
    "print(f\"Training features: {X.shape}\")\n",
    "print(f\"Training labels: {y.shape}\")\n",
    "print(f\"Test features: {X_test.shape}\")\n",
    "print(f\"Test labels: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed45d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267efa94",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "model_knn = KNeighborsClassifier(n_neighbors=5)\n",
    "model_mlp = MLPClassifier(\n",
    "    hidden_layer_sizes=(512, 256),\n",
    "    activation=\"relu\",\n",
    "    solver=\"adam\",\n",
    "    max_iter=5000,\n",
    "    random_state=42,\n",
    "    learning_rate_init=0.001,\n",
    "    early_stopping=True,\n",
    ")\n",
    "model_rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8203ca63",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_knn.fit(X, y)\n",
    "model_mlp.fit(X, y)\n",
    "model_rf.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356bc9ef",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "A confusion matrix is a table used to evaluate the performance of a classification model by comparing its predicted labels to the actual labels, showing how many predictions were correct and where errors occurred. Each row typically represents the actual class, and each column the predicted class, with the diagonal cells indicating correct predictions and off-diagonal cells showing misclassifications. This visualization helps identify not just overall accuracy but also specific types of errors, such as false positives and false negatives, enabling deeper analysis and improvement of the model.\n",
    "\n",
    "<img src=\"../images/confusion_matrix.png\" alt=\"Confusion Matrix\" width=\"400\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c927f74b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_dict = {\n",
    "    0: \"air_conditioner\",\n",
    "    1: \"car_horn\",\n",
    "    2: \"children_playing\",\n",
    "    3: \"dog_bark\",\n",
    "    4: \"drilling\",\n",
    "    5: \"engine_idling\",\n",
    "    6: \"gun_shot\",\n",
    "    7: \"jackhammer\",\n",
    "    8: \"siren\",\n",
    "    9: \"street_music\",\n",
    "}\n",
    "\n",
    "def evaluate_model(model, X_test, y_test):\n",
    "    \"\"\"Evaluates the model and prints the classification report and confusion matrix\"\"\"\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=list(class_dict.values()),\n",
    "        yticklabels=list(class_dict.values()),\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceef6870",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate_model(model_knn, X_test, y_test)\n",
    "# evaluate_model(model_mlp, X_test, y_test)\n",
    "# evaluate_model(model_rf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b426f3d9",
   "metadata": {},
   "source": [
    "## UMAP\n",
    "\n",
    "Unsupervised method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cb73333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    random_state=42, \n",
    "    n_neighbors=5, \n",
    "    min_dist=0.5, \n",
    "    n_components=2, \n",
    "    verbose=True)\n",
    "\n",
    "embedding = reducer.fit_transform(audio_dict[\"feature\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88e406f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datamapplot\n",
    "\n",
    "plot = datamapplot.create_interactive_plot(\n",
    "    embedding,\n",
    "    audio_dict[\"class\"],\n",
    "    hover_text=audio_dict[\"class\"],\n",
    ")\n",
    "plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb40355",
   "metadata": {},
   "source": [
    "## CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8314ed7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata = metadata[metadata[\"fold\"].isin(range(1, 9))]\n",
    "test_metadata = metadata[metadata[\"fold\"].isin([9, 10])]\n",
    "\n",
    "print(f\"Training set: {len(train_metadata)} examples\")\n",
    "print(f\"Test set: {len(test_metadata)} examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11908c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c956ebaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_utils import SimpleCNN, train_model\n",
    "from data_utils import AudioDataset, SpectrogramAugmentation\n",
    "from config import (\n",
    "    BATCH_SIZE,\n",
    "    NUMBER_WORKERS,\n",
    "    EPOCHS,\n",
    "    LEARNING_RATE,\n",
    "    EARLY_STOPPING_PATIENCE,\n",
    "    SCHEDULER_STEP_SIZE,\n",
    "    SCHEDULER_GAMMA,\n",
    "    NUM_CLASSES\n",
    ")\n",
    "\n",
    "train_dataset = AudioDataset(\n",
    "    train_metadata, data_path, transform=SpectrogramAugmentation()\n",
    ")\n",
    "test_dataset = AudioDataset(test_metadata, data_path, transform=None)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUMBER_WORKERS\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUMBER_WORKERS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2c3d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_cnn = SimpleCNN().to(device)\n",
    "simple_cnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83b17f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(simple_cnn.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=SCHEDULER_STEP_SIZE, gamma=SCHEDULER_GAMMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9c4346",
   "metadata": {},
   "outputs": [],
   "source": [
    "model, train_losses, test_losses, train_accuracies, test_accuracies = train_model(\n",
    "    simple_cnn,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    criterion,\n",
    "    optimizer,\n",
    "    device,\n",
    "    num_epochs=EPOCHS,\n",
    "    patience=EARLY_STOPPING_PATIENCE,\n",
    "  # Number of epochs to wait for improvement\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d59b9f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.plot(test_losses, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Test Loss\")\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label=\"Train Accuracy\")\n",
    "plt.plot(test_accuracies, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Accuracy (%)\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Test Accuracy\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9fbf035",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, num_classes=NUM_CLASSES):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc=\"Evaluating\"):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = outputs.max(1)\n",
    "\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    print(\"Classification Report:\")\n",
    "    print(\n",
    "        classification_report(\n",
    "            all_labels, all_preds, target_names=list(class_dict.values())\n",
    "        )\n",
    "    )\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(\n",
    "        cm,\n",
    "        annot=True,\n",
    "        fmt=\"d\",\n",
    "        cmap=\"Blues\",\n",
    "        xticklabels=list(class_dict.values()),\n",
    "        yticklabels=list(class_dict.values()),\n",
    "    )\n",
    "    plt.xlabel(\"Predicted\")\n",
    "    plt.ylabel(\"True\")\n",
    "    plt.title(\"Confusion Matrix\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188823fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Simple CNN:\")\n",
    "evaluate_model(simple_cnn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f32ab054",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "all_outputs = []\n",
    "all_labels = []\n",
    "for inputs, labels in tqdm(train_loader):\n",
    "    inputs, labels = inputs.to(device), labels.to(device)\n",
    "    outputs = model.feature_extractor(inputs)\n",
    "    all_outputs.append(outputs.detach().cpu().numpy())\n",
    "    all_labels.append(labels.detach().cpu().numpy())\n",
    "\n",
    "all_outputs = np.vstack(all_outputs)\n",
    "all_labels = np.concatenate(all_labels)\n",
    "class_names_labels = [class_dict[label] for label in all_labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb64f501",
   "metadata": {},
   "outputs": [],
   "source": [
    "import umap\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    random_state=42, n_neighbors=5, min_dist=0.5, n_components=2, verbose=True\n",
    ")\n",
    "\n",
    "embedding_post_training = reducer.fit_transform(all_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814ba37b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot = datamapplot.create_interactive_plot(\n",
    "    embedding_post_training,\n",
    "    class_names_labels,\n",
    "    hover_text=class_names_labels,\n",
    ")\n",
    "plot"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p312_general",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
