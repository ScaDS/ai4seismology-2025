{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc525df9",
   "metadata": {},
   "source": [
    "# **Seismic Event Classification using Deep Learning**\n",
    "\n",
    "### **Authors**: Akash Kharita and Marine Denolle  (mdenolle@uw.edu)\n",
    "### **Last Updated**: May 6, 2025 \n",
    "\n",
    "---\n",
    "\n",
    "## **Purpose**\n",
    "This notebook trains a CNN classifier for a multi-class seismic event classification of the followin event types: \n",
    "- Earthquakes\n",
    "- Explosions\n",
    "- Surface events\n",
    "- Noise\n",
    "\n",
    "The notebook performs the following tasks:\n",
    "1. Prepares input data (waveform and spectrogram).\n",
    "2. Trains multiple deep learning models.\n",
    "3. Evaluates the models using metrics such as loss, accuracy, confusion matrices, and classification reports.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61af6153",
   "metadata": {},
   "source": [
    "Adding a few missing dependencies.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f39b5264",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install seaborn scikit-learn torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16bfe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable autoreload\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "# from tqdm import tqdm\n",
    "from glob import glob\n",
    "# import time\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "\n",
    "from scipy import stats,signal\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import random_split\n",
    "import numpy as np\n",
    "import scipy.signal as signal\n",
    "\n",
    "# Check if a GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from utils import QuakeXNet_1d,QuakeXNet_2d,SeismicCNN_1d,SeismicCNN_2d\n",
    "from utils import extract_waveforms\n",
    "from utils import extract_spectrograms\n",
    "from utils import PNWDataSet\n",
    "from utils import plot_model_training\n",
    "from utils import train_model\n",
    "from utils import plot_confusion_matrix_and_cr\n",
    "\n",
    "\n",
    "os.makedirs('plot', exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a217d840",
   "metadata": {},
   "source": [
    "# Key Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6530990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## seismic data parameters\n",
    "num_channels = 3 # number of channels\n",
    "fs = 50 # new sampling rate\n",
    "highcut = 20 # (Hz) high cut frequency\n",
    "lowcut = 1 # (Hz) low cut frequency\n",
    "input_window_length = 100 # (seconds)\n",
    "start = -20 # (seconds) to offset the start time from the P wave\n",
    "\n",
    "## Training parameters\n",
    "train_split = 70                                      \n",
    "val_split=20\n",
    "test_split = 10\n",
    "learning_rate=0.001\n",
    "batch_size=128\n",
    "n_epochs=30\n",
    "dropout=0.4\n",
    "criterion=nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aa877e",
   "metadata": {},
   "source": [
    "## Data Download\n",
    "downloads metadata for seismic event classes from CSV files:\n",
    "- `comcat_metadata`: Metadata for earthquake and explosion events.\n",
    "- `exotic_metadata`: Metadata for surface events.\n",
    "- `noise_metadata`: Metadata for noise samples.\n",
    "\n",
    "The data is stored on a shared storage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c51aa71",
   "metadata": {},
   "outputs": [],
   "source": [
    "datadir = '/data/horse/ws/s4122485-ai4seismology/data/earthquake_seismology/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d97869",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data files\n",
    "file_noise = datadir + \"noise_waveforms.hdf5\";\n",
    "file_comcat = datadir + \"mesoPNW_waveforms.hdf5\";\n",
    "file_su = datadir + \"exotic_waveforms.hdf5\";\n",
    "\n",
    "# metadata\n",
    "# accessing the comcat metadata\n",
    "metadata_comcat = pd.read_csv(datadir+\"mesoPNW_metadata.csv\")\n",
    "metadata_su = pd.read_csv(datadir+\"exotic_metadata.csv\")\n",
    "metadata_noise = pd.read_csv(datadir+\"noise_metadata.csv\")\n",
    "# concatenate all dataframes\n",
    "metadata = pd.concat([metadata_comcat, metadata_noise, metadata_su], ignore_index=True)\n",
    "\n",
    "# only select BH channels\n",
    "# metadata = metadata[metadata['station_channel_code'].str.contains('BH')]\n",
    "\n",
    "# creating individual data frames for each class\n",
    "df_exp = metadata[metadata['source_type'] == 'explosion']\n",
    "df_eq = metadata[metadata['source_type'] == 'earthquake']\n",
    "df_su = metadata[metadata['source_type'] == 'surface event']\n",
    "df_no = metadata[metadata['source_type'] == 'noise']\n",
    "df_no['event_id'] = [df_no['trace_start_time'].values[i]+'_noise' for i in range(len(df_no))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de5ceab",
   "metadata": {},
   "outputs": [],
   "source": [
    "number_data_per_class = np.min([len(metadata_comcat[metadata_comcat['source_type'] == 'explosion']), len(metadata_comcat[metadata_comcat['source_type'] == 'earthquake']), len(metadata_su[metadata_su['source_type'] == 'surface event']), len(metadata_noise[metadata_noise['source_type'] == 'noise'])])\n",
    "print('Number of data per class: ', number_data_per_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34434989",
   "metadata": {},
   "source": [
    "We will choose a balanced data set. Unfortunately, there are way fewer examples of surface events compared to other event types."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfec648e",
   "metadata": {},
   "source": [
    "## Extracting Waveform Data\n",
    "Waveforms are extracted from `.hdf5` files using the `extract_waveforms` function. Key parameters:\n",
    "- **Number of channels**: 3 (e.g., Z, N, E components)\n",
    "- **Sampling rate**: 50 Hz\n",
    "- **Window length**: 100 samples\n",
    "- **Bandpass filter**: 1â€“20 Hz\n",
    "\n",
    "Each event class (earthquakes, explosions, surface events, noise) is balanced to contain an equal number of samples.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a943bcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# surface events\n",
    "d_su, id_su = extract_waveforms(df_su, file_su, input_window_length = input_window_length, fs=fs,\n",
    "                                start =start, number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                shifting = True, lowcut = lowcut , highcut =highcut)\n",
    "print(d_su.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258f7ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# noise\n",
    "d_noise, id_noise = extract_waveforms(df_no, file_noise, input_window_length = input_window_length, fs=fs,\n",
    "                                start =start, number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                shifting = True, lowcut = lowcut , highcut =highcut)\n",
    "print(d_noise.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0121b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# explosions\n",
    "d_exp, id_exp = extract_waveforms(df_exp, file_comcat, input_window_length = input_window_length, fs=fs,\n",
    "                                start =start, number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                shifting = True, lowcut = lowcut , highcut =highcut)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bdf158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# earthquakes\n",
    "d_eq, id_eq = extract_waveforms(df_eq, file_comcat, input_window_length = input_window_length, fs=fs,\n",
    "                                start =start, number_data = number_data_per_class, num_channels = num_channels,\n",
    "                                shifting = True, lowcut = lowcut , highcut =highcut)\n",
    "print(d_eq.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abe1b4c",
   "metadata": {},
   "source": [
    "Now that we have four independent datasets, we will combine them to form a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e9be38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate all data to make a single X data set, and prepare one-hot encoding of y for the classes on source type\n",
    "X = np.concatenate((d_eq,d_exp, d_su,d_noise), axis=0)\n",
    "y = np.concatenate((np.zeros(d_eq.shape[0]), np.ones(d_exp.shape[0]), 2*np.ones(d_su.shape[0]), 3*np.ones(d_noise.shape[0])), axis=0)\n",
    "y = y.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f50cd025",
   "metadata": {},
   "source": [
    "# Prepare Pytorch data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9df2fa",
   "metadata": {},
   "source": [
    "## 1. Time series as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36a92bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the data a PNWDataSet\n",
    "custom_dataset = PNWDataSet(X,y,4)\n",
    "# print the shape of the dataset\n",
    "print('Shape of the dataset: ', custom_dataset.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92dc9e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare training and validation sets\n",
    "train_size = int(len(custom_dataset) * 0.7)\n",
    "val_size = int(len(custom_dataset) * 0.2)\n",
    "test_size = len(custom_dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(custom_dataset, [train_size, val_size, test_size])\n",
    "# Create data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "# Check the number of samples in each set\n",
    "print(f\"Number of samples in training set: {len(train_dataset)}\")\n",
    "print(f\"Number of samples in validation set: {len(val_dataset)}\")\n",
    "print(f\"Number of samples in test set: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a81afd",
   "metadata": {},
   "source": [
    "## 2. Spectrograms as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "078ead5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate spectrograms of all X\n",
    "X_spectrograms=extract_spectrograms(X,fs=fs)\n",
    "print(X_spectrograms.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2990c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create costum dataset for spectrograms\n",
    "custom_dataset_spectrograms = PNWDataSet(X_spectrograms,y,4)\n",
    "# prepare training and validation sets\n",
    "train_size = int(len(custom_dataset_spectrograms) * 0.7)\n",
    "val_size = int(len(custom_dataset_spectrograms) * 0.2)\n",
    "test_size = len(custom_dataset_spectrograms) - train_size - val_size\n",
    "train_dataset_spectrograms, val_dataset_spectrograms, test_dataset_spectrograms = random_split(custom_dataset_spectrograms, [train_size, val_size, test_size])\n",
    "# Create data loaders\n",
    "train_loader_spectrograms = torch.utils.data.DataLoader(train_dataset_spectrograms, batch_size=batch_size, shuffle=True)\n",
    "val_loader_spectrograms = torch.utils.data.DataLoader(val_dataset_spectrograms, batch_size=batch_size, shuffle=True)\n",
    "test_loader_spectrograms = torch.utils.data.DataLoader(test_dataset_spectrograms, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8ce3e6",
   "metadata": {},
   "source": [
    "# Model training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7955b0e0",
   "metadata": {},
   "source": [
    "## SeismicCNN 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d1a8b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a model\n",
    "model1 = SeismicCNN_1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5722547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train the model\n",
    "\n",
    "from utils import train_model\n",
    "(train_loss1, val_loss1, val_acc1,training_time, test_loss,test_acc) = train_model(model1,\n",
    "    train_loader,  \n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    n_epochs=n_epochs,\n",
    "    learning_rate=learning_rate,\n",
    "    criterion=criterion,\n",
    "    augmentation= False, \n",
    "    patience = 30, \n",
    "    model_path = 'trained_models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2b808a",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_training(train_loss1, val_loss1, val_acc1,test_loss,test_acc)\n",
    "plot_confusion_matrix_and_cr(model1, test_loader, \n",
    "    classes = ['earthquake', 'explosion', 'surface event', 'noise'], \n",
    "    fname = 'plot/confusion_matrix_seismic_cnn1.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff59affa",
   "metadata": {},
   "source": [
    "## SeismicCNN 2D\n",
    "\n",
    "This is a long skinny neural network that takes spectrograms as inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5155697b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a modelÃŸ\n",
    "model2 = SeismicCNN_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9092b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_loss2, val_loss2, val_acc2,training_time, test_loss,test_acc) = train_model(model2,\n",
    "    train_loader_spectrograms,  \n",
    "    val_loader_spectrograms,\n",
    "    test_loader_spectrograms,\n",
    "    n_epochs=20,\n",
    "    learning_rate=learning_rate,\n",
    "    criterion=criterion,\n",
    "    augmentation= False, \n",
    "    patience = 30, \n",
    "    model_path = 'trained_model_seismiccnn_2d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66b021b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_training(train_loss2, val_loss2, val_acc2,test_loss,test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd94fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_confusion_matrix_and_cr(model2,test_loader_spectrograms)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c8e1701",
   "metadata": {},
   "source": [
    "## QuakeXNet 1D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac566cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = QuakeXNet_1d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd39fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_loss3, val_loss3, val_acc3,training_time, test_loss,test_acc) = train_model(model3,\n",
    "    train_loader,  \n",
    "    val_loader,\n",
    "    test_loader,\n",
    "    n_epochs=20,\n",
    "    learning_rate=learning_rate,\n",
    "    criterion=criterion,\n",
    "    augmentation= False, \n",
    "    patience = 30, \n",
    "    model_path = 'trained_model_quakeXNet_1d')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d152ce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_training(train_loss3, val_loss3, val_acc3,test_loss,test_acc,title=\"QuakeXNet (1D)\")\n",
    "plot_confusion_matrix_and_cr(model3,test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4040e51c",
   "metadata": {},
   "source": [
    "## QuakeXNet 2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c3936a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model4 = QuakeXNet_2d()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146a428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_loss4, val_loss4, val_acc4,training_time, test_loss,test_acc) = train_model(model4,\n",
    "    train_loader_spectrograms,  \n",
    "    val_loader_spectrograms,\n",
    "    test_loader_spectrograms,\n",
    "    n_epochs=20,\n",
    "    learning_rate=learning_rate,\n",
    "    criterion=criterion,\n",
    "    augmentation= False, \n",
    "    patience = 30, \n",
    "    model_path = 'trained_model_quakeXNet_2d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cca20d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model_training(train_loss4, val_loss4, val_acc4,test_loss,test_acc,title=\"QuakeXNet (2D)\")\n",
    "plot_confusion_matrix_and_cr(model4,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8dd6165",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Print the summary\n",
    "import torchsummary\n",
    "print('SeismicCNN(1D) Architecture')\n",
    "torchsummary.summary(model1, input_size=(3,5000))\n",
    "\n",
    "print('SeismicCNN(2D) Architecture')\n",
    "torchsummary.summary(model2, input_size=(3,129,38))\n",
    "\n",
    "print('QuakeXNet (1D) Architecture')\n",
    "torchsummary.summary(model3, input_size=(3,5000))\n",
    "\n",
    "print('QuakeXNet(2D) Architecture')\n",
    "torchsummary.summary(model4, input_size=(3,129,38))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlgeo",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
